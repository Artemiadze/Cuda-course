# CUDA Streams Examples

## Intuition
Потоки можно рассматривать как "речные потоки", в которых направление операций изменяется только во времени (как на временной шкале). Например, скопируйте некоторые данные (временной шаг 1), затем выполните некоторые вычисления (временной шаг 2), затем скопируйте некоторые данные обратно (временной шаг 3). Это основная идея, лежащая в основе потоков. 

В CUDA у нас может быть несколько потоков одновременно, и у каждого потока может быть своя временная шкала. Это позволяет нам перекрывать операции и лучше использовать графический процессор.

При обучении большой языковой модели было бы глупо тратить массу времени на загрузку всех токенов в графический процессор и из него. Потоки позволяют нам перемещать данные, одновременно выполняя вычисления в любое время. Потоки представляют собой программную абстракцию, называемую "предварительной выборкой", которая позволяет перемещать данные до того, как они понадобятся. Это способ скрыть задержку при перемещении данных. 

Этот проект демонстрирует использование потоков CUDA для параллельного выполнения и лучшей загрузки графического процессора. Он содержит два примера:


## Code Snippets
- default **stream** = **stream** 0 = null **stream**
```cpp
// This kernel launch uses the null stream (0)
myKernel<<<gridSize, blockSize>>>(args);

// This is equivalent to
myKernel<<<gridSize, blockSize, 0, 0>>>(args);
```
Помните эту часть из раздела о ядрах?
- Конфигурация выполнения (вызова глобальной функции) задается путем вставки выражения вида `<<<gridDim, blockDim, Ns, S>>>`, где:

  - Dg (dim3) определяет размер сетки.
  - Db (dim3) определяет размер каждого блока.
  - Ns (size_t) указывает количество байт в общей памяти, которое динамически выделяется для каждого блока для этого вызова в дополнение к статически выделяемой памяти. (обычно не указывается)
  - S (cudaStream_t) указывает связанный поток, это необязательный параметр, значение которого по умолчанию равно 0.

- поток 1 и поток 2 создаются с разными приоритетами. это означает, что они выполняются в определенном порядке во время выполнения. по сути, это дает нам больше контроля над параллельным выполнением наших ядер.

```cpp
    // Create streams with different priorities
    int leastPriority, greatestPriority;
    CHECK_CUDA_ERROR(cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority));
    CHECK_CUDA_ERROR(cudaStreamCreateWithPriority(&stream1, cudaStreamNonBlocking, leastPriority));
    CHECK_CUDA_ERROR(cudaStreamCreateWithPriority(&stream2, cudaStreamNonBlocking, greatestPriority));
```

## Examples

1. `stream_basics.cu`: Иллюстрирует базовое использование потока при асинхронной передаче памяти и запуске ядра.
2. `stream_advanced.cu`: Демонстрирует более продвинутые концепции, такие как приоритеты потоков, обратные вызовы и межпотоковые зависимости.

## Compilation

Чтобы скомпилировать примеры, используйте следующие команды:

```bash
nvcc -o 01 01_stream_basics.cu
nvcc -o 02 02_stream_advanced.cu
```

## Docs
- https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf

## Pinned Memory
- "это понадобится нам позже, так что не играйте с этим" - вот хороший повод задуматься об этом.
- закрепленная память - это память, которая зафиксирована на месте и не может быть перемещена операционной системой. Это полезно, когда вы хотите переместить данные в графический процессор и выполнить с ними некоторые вычисления. Если операционная система перемещает данные, графический процессор будет искать данные не в том месте, и вы получите ошибку сегмента.
```cpp
// Allocate pinned memory
float* h_data;
cudaMallocHost((void**)&h_data, size);
```

## Events
- Измерение времени выполнения ядра: события размещаются до и после запуска ядра для точного измерения времени выполнения.

- Синхронизация между потоками: События могут использоваться для создания зависимостей между различными потоками, гарантируя, что одна операция начнется только после завершения другой.

- Дублирование вычислений и передачи данных: События могут отмечать завершение передачи данных, сигнализируя о том, что вычисления могут начаться с использованием этих данных.that computation can begin on that data.


```cpp
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start, stream);
kernel<<<grid, block, 0, stream>>>(args);
cudaEventRecord(stop, stream);

cudaEventSynchronize(stop);
float milliseconds = 0;
cudaEventElapsedTime(&milliseconds, start, stop);
```

## Callbacks
- Используя обратные вызовы, вы можете настроить конвейер, в котором завершение одной операции на графическом процессоре запускает другую операцию на центральном процессоре, которая затем может поставить в очередь дополнительную работу для графического процессора. (как показано в документации по параллелизму nvidia, приведенной выше)

```cpp
void CUDART_CB MyCallback(cudaStream_t stream, cudaError_t status, void *userData) {
    printf("GPU operation completed\n");
    // Trigger next batch of work
}

kernel<<<grid, block, 0, stream>>>(args);
cudaStreamAddCallback(stream, MyCallback, nullptr, 0);
```
